# ìµœì§€ì˜_ë…¼ë¬¸ë¶„ì„   2025ë…„ 7ì›”2ì¼ì¼
import streamlit as st
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
import numpy as np
import re
import io
import base64
import matplotlib.font_manager as fm
import warnings
import os
warnings.filterwarnings('ignore')

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="ì—°êµ¬ì ê´€ê³„ë„ ë¶„ì„ ì‹œìŠ¤í…œ",
    page_icon="ğŸ“Š",
    layout="wide"
)

# í•œê¸€ í°íŠ¸ ì„¤ì • ê°œì„ 
def setup_korean_font():
    """í•œê¸€ í°íŠ¸ ì„¤ì •"""
    try:
        # Windows í•œê¸€ í°íŠ¸ ê²½ë¡œ ì§ì ‘ ì§€ì •
        font_paths = [
            'C:/Windows/Fonts/malgun.ttf',  # ë§‘ì€ ê³ ë”•
            'C:/Windows/Fonts/gulim.ttc',   # êµ´ë¦¼
            'C:/Windows/Fonts/dotum.ttc',   # ë‹ì›€
            'C:/Windows/Fonts/batang.ttc',  # ë°”íƒ•
        ]
        
        for font_path in font_paths:
            try:
                if os.path.exists(font_path):
                    font_prop = fm.FontProperties(fname=font_path)
                    plt.rcParams['font.family'] = font_prop.get_name()
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"í•œê¸€ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_path}")
                    return font_prop
            except:
                continue
        
        # í°íŠ¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ì‹œìŠ¤í…œ í°íŠ¸ì—ì„œ ì°¾ê¸°
        font_list = ['Malgun Gothic', 'NanumGothic', 'Gulim', 'Dotum', 'Batang']
        for font_name in font_list:
            try:
                font_prop = fm.FontProperties(family=font_name)
                if font_prop.get_name() != 'DejaVu Sans':
                    plt.rcParams['font.family'] = font_name
                    plt.rcParams['axes.unicode_minus'] = False
                    print(f"ì‹œìŠ¤í…œ í°íŠ¸ ì„¤ì • ì™„ë£Œ: {font_name}")
                    return font_prop
            except:
                continue
        
        # ë§ˆì§€ë§‰ ìˆ˜ë‹¨: ê¸°ë³¸ ì„¤ì •
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['axes.unicode_minus'] = False
        print("ê¸°ë³¸ í°íŠ¸ ì„¤ì • ì‚¬ìš©")
        return None
        
    except Exception as e:
        print(f"í°íŠ¸ ì„¤ì • ì˜¤ë¥˜: {e}")
        plt.rcParams['font.family'] = 'sans-serif'
        plt.rcParams['axes.unicode_minus'] = False
        return None

# í°íŠ¸ ì„¤ì • ì‹¤í–‰
font_prop = setup_korean_font()

class ResearchAnalyzer:
    def __init__(self):
        self.data = None
        self.researchers = []
        self.keywords = []
        self.graph = nx.Graph()
        
    def load_data(self, data):
        """ë°ì´í„° ë¡œë“œ"""
        if isinstance(data, list):
            self.data = pd.DataFrame(data)
        else:
            self.data = data
        return self.data
    
    def preprocess_keywords(self, keyword_column):
        """ì£¼ì œì–´ ì „ì²˜ë¦¬"""
        if keyword_column not in self.data.columns:
            raise ValueError(f"ì»¬ëŸ¼ '{keyword_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        self.data['processed_keywords'] = self.data[keyword_column].apply(
            lambda x: [kw.strip().lower() for kw in re.split(r'[,;]', str(x)) if kw.strip()]
        )
        
        all_keywords = []
        for keywords in self.data['processed_keywords']:
            all_keywords.extend(keywords)
        
        self.keywords = all_keywords
        return len(set(all_keywords))
        
    def analyze_researcher_relationships(self, researcher_column):
        """ì—°êµ¬ì ê°„ ê´€ê³„ ë¶„ì„"""
        if researcher_column not in self.data.columns:
            raise ValueError(f"ì»¬ëŸ¼ '{researcher_column}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        researcher_keywords = {}
        for idx, row in self.data.iterrows():
            researchers = str(row[researcher_column]).split(',')
            keywords = row['processed_keywords']
            
            for researcher in researchers:
                researcher = researcher.strip()
                if researcher not in researcher_keywords:
                    researcher_keywords[researcher] = []
                researcher_keywords[researcher].extend(keywords)
        
        self.researchers = list(researcher_keywords.keys())
        
        for i, researcher1 in enumerate(self.researchers):
            for j, researcher2 in enumerate(self.researchers[i+1:], i+1):
                keywords1 = set(researcher_keywords[researcher1])
                keywords2 = set(researcher_keywords[researcher2])
                
                intersection = len(keywords1.intersection(keywords2))
                union = len(keywords1.union(keywords2))
                
                if union > 0:
                    similarity = intersection / union
                    if similarity > 0:
                        self.graph.add_edge(researcher1, researcher2, weight=similarity)
        
        return len(self.graph.edges())
    
    def analyze_keyword_network(self):
        """í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ ë¶„ì„"""
        keyword_graph = nx.Graph()
        
        for keywords in self.data['processed_keywords']:
            for i, kw1 in enumerate(keywords):
                for kw2 in keywords[i+1:]:
                    if keyword_graph.has_edge(kw1, kw2):
                        keyword_graph[kw1][kw2]['weight'] += 1
                    else:
                        keyword_graph.add_edge(kw1, kw2, weight=1)
        
        return keyword_graph

def plot_to_base64(fig):
    """matplotlib ê·¸ë˜í”„ë¥¼ base64ë¡œ ì¸ì½”ë”©"""
    buf = io.BytesIO()
    fig.savefig(buf, format='png', bbox_inches='tight', dpi=300)
    buf.seek(0)
    img_str = base64.b64encode(buf.read()).decode()
    buf.close()
    return img_str

def create_korean_chart(fig, ax, title, xlabel, ylabel):
    """í•œê¸€ ì°¨íŠ¸ ìƒì„± í—¬í¼ í•¨ìˆ˜"""
    # í°íŠ¸ ì¬ì„¤ì •
    setup_korean_font()
    
    ax.set_title(title, fontsize=14, fontweight='bold')
    ax.set_xlabel(xlabel, fontsize=12)
    ax.set_ylabel(ylabel, fontsize=12)
    
    # ì¶• ë ˆì´ë¸” í°íŠ¸ ì„¤ì •
    for label in ax.get_xticklabels() + ax.get_yticklabels():
        label.set_fontsize(10)
    
    plt.tight_layout()

def main():
    st.title("ğŸ“Š ì—°êµ¬ì ê´€ê³„ë„ ë¶„ì„ ì‹œìŠ¤í…œ")
    st.markdown("---")
    
    # ì‚¬ì´ë“œë°” - ë°ì´í„° ì…ë ¥
    st.sidebar.header("ğŸ“‹ ë°ì´í„° ì…ë ¥")
    
    # ìƒ˜í”Œ ë°ì´í„° ì œê³µ
    if st.sidebar.checkbox("ìƒ˜í”Œ ë°ì´í„° ì‚¬ìš©"):
        sample_data = [
            {
                'title': 'ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ì—°êµ¬',
                'researchers': 'ê¹€ì² ìˆ˜, ì´ì˜í¬, ë°•ë¯¼ìˆ˜',
                'keywords': 'ë”¥ëŸ¬ë‹, ì´ë¯¸ì§€ë¶„ë¥˜, CNN, ì»´í“¨í„°ë¹„ì „',
                'year': 2023
            },
            {
                'title': 'ìì—°ì–´ì²˜ë¦¬ ê¸°ë°˜ ê°ì •ë¶„ì„ ì‹œìŠ¤í…œ',
                'researchers': 'ì´ì˜í¬, ìµœì§€ì›',
                'keywords': 'ìì—°ì–´ì²˜ë¦¬, ê°ì •ë¶„ì„, BERT, í…ìŠ¤íŠ¸ë§ˆì´ë‹',
                'year': 2022
            },
            {
                'title': 'ê°•í™”í•™ìŠµì„ ì´ìš©í•œ ê²Œì„ AI ê°œë°œ',
                'researchers': 'ë°•ë¯¼ìˆ˜, ê¹€ì² ìˆ˜',
                'keywords': 'ê°•í™”í•™ìŠµ, ê²Œì„AI, ë”¥ëŸ¬ë‹, Q-learning',
                'year': 2021
            },
            {
                'title': 'ë¹…ë°ì´í„° ë¶„ì„ì„ í†µí•œ ê³ ê° í–‰ë™ ì˜ˆì¸¡',
                'researchers': 'ìµœì§€ì›, ì •ìˆ˜ì§„',
                'keywords': 'ë¹…ë°ì´í„°, ê³ ê°í–‰ë™, ì˜ˆì¸¡ëª¨ë¸, ë¨¸ì‹ ëŸ¬ë‹',
                'year': 2020
            },
            {
                'title': 'ì»´í“¨í„°ë¹„ì „ ê¸°ë°˜ ììœ¨ì£¼í–‰ ì‹œìŠ¤í…œ',
                'researchers': 'ì •ìˆ˜ì§„, ê¹€ì² ìˆ˜',
                'keywords': 'ì»´í“¨í„°ë¹„ì „, ììœ¨ì£¼í–‰, ë”¥ëŸ¬ë‹, ì´ë¯¸ì§€ì²˜ë¦¬',
                'year': 2023
            }
        ]
        
        data = pd.DataFrame(sample_data)
        st.sidebar.success("ìƒ˜í”Œ ë°ì´í„°ê°€ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        
    else:
        # íŒŒì¼ ì—…ë¡œë“œ
        uploaded_file = st.sidebar.file_uploader(
            "CSV íŒŒì¼ ì—…ë¡œë“œ", 
            type=['csv'],
            help="ë…¼ë¬¸ ì œëª©, ì—°êµ¬ì, í‚¤ì›Œë“œ, ë°œí–‰ë…„ë„(year) ì»¬ëŸ¼ì´ í¬í•¨ëœ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì„¸ìš”."
        )
        
        if uploaded_file is not None:
            data = pd.read_csv(uploaded_file)
            st.sidebar.success("íŒŒì¼ì´ ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤!")
        else:
            st.info("ìƒ˜í”Œ ë°ì´í„°ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ CSV íŒŒì¼ì„ ì—…ë¡œë“œí•´ì£¼ì„¸ìš”.")
            return
    
    # ì»¬ëŸ¼ ì„ íƒ
    st.sidebar.header("ğŸ”§ ì„¤ì •")
    
    col1, col2 = st.sidebar.columns(2)
    with col1:
        title_col = st.selectbox("ë…¼ë¬¸ ì œëª© ì»¬ëŸ¼", data.columns.tolist(), index=0)
    with col2:
        researcher_col = st.selectbox("ì—°êµ¬ì ì»¬ëŸ¼", data.columns.tolist(), index=1)
    
    keyword_col = st.sidebar.selectbox("í‚¤ì›Œë“œ ì»¬ëŸ¼", data.columns.tolist(), index=2)
    year_col = st.sidebar.selectbox("ë°œí–‰ë…„ë„ ì»¬ëŸ¼", data.columns.tolist(), index=3 if 'year' in data.columns else 0)
    
    # ë¶„ì„ ì‹¤í–‰ ë²„íŠ¼
    if st.sidebar.button("ğŸš€ ë¶„ì„ ì‹œì‘", type="primary"):
        with st.spinner("ë°ì´í„°ë¥¼ ë¶„ì„í•˜ê³  ìˆìŠµë‹ˆë‹¤..."):
            # ë¶„ì„ê¸° ì´ˆê¸°í™”
            analyzer = ResearchAnalyzer()
            
            # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
            analyzer.load_data(data)
            unique_keywords = analyzer.preprocess_keywords(keyword_col)
            connections = analyzer.analyze_researcher_relationships(researcher_col)
            
            # ê²°ê³¼ í‘œì‹œ
            st.success(f"ë¶„ì„ ì™„ë£Œ! {len(data)}ê°œ ë…¼ë¬¸, {len(analyzer.researchers)}ëª… ì—°êµ¬ì, {unique_keywords}ê°œ í‚¤ì›Œë“œ, {connections}ê°œ ì—°ê²°")
            
            # ë©”ì¸ ì»¨í…ì¸  ì˜ì—­
            col1, col2 = st.columns(2)
            
            with col1:
                st.subheader("ğŸ“ˆ ê¸°ë³¸ í†µê³„")
                
                # í†µê³„ ì¹´ë“œ
                metric_col1, metric_col2, metric_col3 = st.columns(3)
                with metric_col1:
                    st.metric("ë…¼ë¬¸ ìˆ˜", len(data))
                with metric_col2:
                    st.metric("ì—°êµ¬ì ìˆ˜", len(analyzer.researchers))
                with metric_col3:
                    st.metric("í‚¤ì›Œë“œ ìˆ˜", unique_keywords)
                
                # ë°ì´í„° ë¯¸ë¦¬ë³´ê¸° (ë°œí–‰ë…„ë„ í¬í•¨)
                st.subheader("ğŸ“‹ ë°ì´í„° ë¯¸ë¦¬ë³´ê¸°")
                st.dataframe(data[[title_col, researcher_col, keyword_col, year_col]].head(), use_container_width=True)
            
            with col2:
                st.subheader("ğŸ† ì£¼ìš” í‚¤ì›Œë“œ")
                
                # í‚¤ì›Œë“œ ë¹ˆë„ ë¶„ì„
                keyword_counts = Counter(analyzer.keywords)
                top_keywords = keyword_counts.most_common(10)
                
                # í‚¤ì›Œë“œ ì°¨íŠ¸
                fig, ax = plt.subplots(figsize=(8, 6))
                keywords, counts = zip(*top_keywords)
                bars = ax.barh(range(len(keywords)), counts, color='skyblue', alpha=0.7)
                ax.set_yticks(range(len(keywords)))
                ax.set_yticklabels(keywords)
                
                create_korean_chart(fig, ax, 'ìƒìœ„ 10ê°œ í‚¤ì›Œë“œ ë¹ˆë„', 'ë¹ˆë„', 'í‚¤ì›Œë“œ')
                ax.invert_yaxis()
                
                # ê°’ í‘œì‹œ
                for i, (bar, count) in enumerate(zip(bars, counts)):
                    ax.text(bar.get_width() + 0.1, bar.get_y() + bar.get_height()/2, 
                           str(count), ha='left', va='center')
                
                st.pyplot(fig)
            
            # ì—°êµ¬ì ê´€ê³„ë„
            st.subheader("ğŸ‘¥ ì—°êµ¬ì ê´€ê³„ë„")
            
            if len(analyzer.graph.nodes()) > 0:
                fig, ax = plt.subplots(figsize=(12, 8))
                
                # ë…¸ë“œ í¬ê¸° (ì—°ê²° ìˆ˜ì— ë¹„ë¡€)
                node_sizes = [analyzer.graph.degree(node) * 200 for node in analyzer.graph.nodes()]
                
                # ì—£ì§€ ë‘ê»˜ (ìœ ì‚¬ë„ì— ë¹„ë¡€)
                edge_weights = [analyzer.graph[u][v]['weight'] * 5 for u, v in analyzer.graph.edges()]
                
                # ë ˆì´ì•„ì›ƒ ì„¤ì •
                pos = nx.spring_layout(analyzer.graph, k=2, iterations=50)
                
                # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
                nx.draw_networkx_nodes(analyzer.graph, pos, 
                                     node_size=node_sizes, 
                                     node_color='lightblue',
                                     alpha=0.8)
                
                nx.draw_networkx_edges(analyzer.graph, pos, 
                                     width=edge_weights,
                                     alpha=0.6,
                                     edge_color='gray')
                
                # í•œê¸€ í°íŠ¸ ì„¤ì • í›„ ë¼ë²¨ ê·¸ë¦¬ê¸°
                if font_prop:
                    nx.draw_networkx_labels(analyzer.graph, pos, 
                                          font_size=12,
                                          font_weight='bold',
                                          font_family=font_prop.get_name())
                else:
                    nx.draw_networkx_labels(analyzer.graph, pos, 
                                          font_size=12,
                                          font_weight='bold')
                
                create_korean_chart(fig, ax, "ì—°êµ¬ì ê°„ ê´€ê³„ë„ (í‚¤ì›Œë“œ ìœ ì‚¬ë„ ê¸°ë°˜)", "", "")
                plt.axis('off')
                st.pyplot(fig)
                
                # ì—°êµ¬ìë³„ ìƒì„¸ ì •ë³´
                st.subheader("ğŸ” ì—°êµ¬ìë³„ ìƒì„¸ ì •ë³´")
                
                researcher_info = []
                for researcher in analyzer.researchers:
                    degree = analyzer.graph.degree(researcher)
                    connections = [neighbor for neighbor in analyzer.graph.neighbors(researcher)]
                    researcher_info.append({
                        'ì—°êµ¬ì': researcher,
                        'ì—°ê²° ìˆ˜': degree,
                        'ì—°ê²°ëœ ì—°êµ¬ì': ', '.join(connections) if connections else 'ì—†ìŒ'
                    })
                
                researcher_df = pd.DataFrame(researcher_info)
                researcher_df = researcher_df.sort_values('ì—°ê²° ìˆ˜', ascending=False)
                st.dataframe(researcher_df, use_container_width=True)
                
            else:
                st.warning("ì—°êµ¬ì ê°„ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬
            st.subheader("ğŸ·ï¸ í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬")
            
            min_weight = st.slider("ìµœì†Œ ì—°ê²° ê°•ë„", min_value=1, max_value=5, value=1)
            
            keyword_graph = analyzer.analyze_keyword_network()
            filtered_edges = [(u, v, d) for u, v, d in keyword_graph.edges(data=True) 
                             if d['weight'] >= min_weight]
            
            if filtered_edges:
                fig, ax = plt.subplots(figsize=(12, 8))
                
                filtered_graph = nx.Graph()
                # ê°€ì¤‘ì¹˜ë¥¼ ìˆ«ìë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
                for u, v, d in filtered_edges:
                    filtered_graph.add_edge(u, v, weight=float(d['weight']))
                
                # ë…¸ë“œ í¬ê¸° (ì—°ê²° ìˆ˜ì— ë¹„ë¡€)
                node_sizes = [filtered_graph.degree(node) * 100 for node in filtered_graph.nodes()]
                
                # ì—£ì§€ ë‘ê»˜ (ê°€ì¤‘ì¹˜ì— ë¹„ë¡€)
                edge_weights = [filtered_graph[u][v]['weight'] for u, v in filtered_graph.edges()]
                
                # ë ˆì´ì•„ì›ƒ ì„¤ì •
                pos = nx.spring_layout(filtered_graph, k=3, iterations=100)
                
                # ê·¸ë˜í”„ ê·¸ë¦¬ê¸°
                nx.draw_networkx_nodes(filtered_graph, pos, 
                                     node_size=node_sizes, 
                                     node_color='lightcoral',
                                     alpha=0.8)
                
                nx.draw_networkx_edges(filtered_graph, pos, 
                                     width=edge_weights,
                                     alpha=0.6,
                                     edge_color='red')
                
                # í‚¤ì›Œë“œ ë¼ë²¨ì—ë„ í•œê¸€ í°íŠ¸ ì ìš©
                if font_prop:
                    nx.draw_networkx_labels(filtered_graph, pos, 
                                          font_size=10,
                                          font_weight='bold',
                                          font_family=font_prop.get_name())
                else:
                    nx.draw_networkx_labels(filtered_graph, pos, 
                                          font_size=10,
                                          font_weight='bold')
                
                create_korean_chart(fig, ax, f"í‚¤ì›Œë“œ ë„¤íŠ¸ì›Œí¬ (ìµœì†Œ ì—°ê²° ê°•ë„: {min_weight})", "", "")
                plt.axis('off')
                st.pyplot(fig)
                
            else:
                st.warning(f"ê°€ì¤‘ì¹˜ {min_weight} ì´ìƒì˜ í‚¤ì›Œë“œ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸
            st.subheader("ğŸ“Š ìƒì„¸ ë¶„ì„ ë¦¬í¬íŠ¸")
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.markdown("**ğŸ”— ì—°ê²° ê°•ë„ë³„ ë¶„í¬**")
                if analyzer.graph.edges():
                    weights = [analyzer.graph[u][v]['weight'] for u, v in analyzer.graph.edges()]
                    fig, ax = plt.subplots(figsize=(8, 6))
                    ax.hist(weights, bins=10, color='lightgreen', alpha=0.7, edgecolor='black')
                    create_korean_chart(fig, ax, 'ì—°êµ¬ì ê°„ ì—°ê²° ê°•ë„ ë¶„í¬', 'ì—°ê²° ê°•ë„ (Jaccard ìœ ì‚¬ë„)', 'ì—°ê²° ìˆ˜')
                    st.pyplot(fig)
            
            with col2:
                st.markdown("**ğŸ“ˆ í‚¤ì›Œë“œ ì‚¬ìš© íŒ¨í„´**")
                keyword_freq = pd.DataFrame(keyword_counts.most_common(20), 
                                          columns=['í‚¤ì›Œë“œ', 'ë¹ˆë„'])
                fig, ax = plt.subplots(figsize=(8, 6))
                ax.plot(keyword_freq['í‚¤ì›Œë“œ'], keyword_freq['ë¹ˆë„'], 
                       marker='o', color='orange', linewidth=2, markersize=6)
                create_korean_chart(fig, ax, 'í‚¤ì›Œë“œ ë¹ˆë„ ë¶„í¬', 'í‚¤ì›Œë“œ', 'ë¹ˆë„')
                plt.xticks(rotation=45, ha='right')
                st.pyplot(fig)

if __name__ == "__main__":
    main() 